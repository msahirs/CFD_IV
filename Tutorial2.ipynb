{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Surrogate modelling with Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major issue in Uncertainty Quantification is the high-cost of evaluating the computational model one time (e.g. a CFD code) - combined with the large number of evaluations needed.  *Surrogate modelling* is a general term, describing replacing the CFD code with a cheap map that approximates it.  The polynomial stochastic methods are also surrogate models when used for interpolation (as opposed to quadrature).\n",
    "\n",
    "With $d \\in\\mathbb{N}$ the number of parameters (the *dimension*), and a scalar output, any simulation code $g(\\cdot)$ is schematically:\n",
    "$$\n",
    "g: \\mathbb{R}^d\\rightarrow \\mathbb{R}\n",
    "$$\n",
    "to be approximated by a surrogate model $\\hat g(\\cdot)$:\n",
    "$$\n",
    "g(\\mathbf{x}) \\simeq \\hat g(\\mathbf{x}),\\quad \\mathbf{x}\\in\\Omega \\subset \\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "Surrogate models can be interpolative or regressive.  Some examples:\n",
    "\n",
    "* Stochastic polynomial interpolants, including sparse-grids;\n",
    "* Linear regression;\n",
    "* Radial-basis function interpolation.\n",
    "\n",
    "Our goal in this tutorial is to implement and explore the *stochastic* regressive surrogate model $\\hat g$ called variously \"Kriging\" (by engineers) or \"Gaussian Process Regression\" (by mathematicians).  For simplicity will will consider the case $d=1$, but the generalization to higher-dimensions is striaghtforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the test-function\n",
    "$$\n",
    "g(x) = \\sin(cx^2)\\cdot (1+x) + (1+x^2),\\quad c=40,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate as integrate\n",
    "import scipy.interpolate as interpolate\n",
    "import scipy.stats as stats\n",
    "\n",
    "c = 40\n",
    "def g(x): return np.sin(x*x*c)*(1+x) + (1+x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 201)\n",
    "plt.plot(x, g(x), '-')\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$g(\\cdot)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the domain $\\Omega$\n",
    "**EXERCISE 1**\n",
    "\n",
    "Generate about $M=10$ *training nodes* randomly on $\\Omega = [0,1]$ (e.g. with `np.random.rand()`), and additionally about $N=100$ *test nodes* uniformly spaced on $\\Omega = [0,1]$.  Concatenate these arrays (e.g. with `np.hstack()`) to get a single array of both training and test nodes, \"all\".  To check your code `x_all.shape == (M+N,)` should be `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =\n",
    "N =\n",
    "\n",
    "x_train = \n",
    "x_test = \n",
    "x_all = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is computing the prior covariance matrix based on the definition in the notes:\n",
    "$$\n",
    "[P]_{ij} := \\sigma_y^2 r(x_i, x_j, l),\\quad r(x_i,x_j,l) := \\exp\\left\\{-\\frac{|x_i-x_j|^2}{2 l^2}\\right\\}\n",
    "$$\n",
    "In the following code `r()` with array arguments `xi` and `xj`, computes the function $r()$ above, for all combinations of array entries, and returns the result in a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_all(xi,xj): \n",
    "    return np.abs(xi[np.newaxis,:] - xj[:,np.newaxis])\n",
    "\n",
    "def r(xi, xj, sigma_0, l): \n",
    "    return sigma_0**2 * np.exp(-distances_all(xi,xj)**2/ l**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2**\n",
    "\n",
    "Consider the Gaussian process $\\mathcal{Y}_0$ defined by some mean function $\\mu(x)$ and $r()$, written:\n",
    "$$\n",
    "\\mathcal{Y}_0 \\sim \\mathcal{GP}(\\mu, r).\n",
    "$$\n",
    "We know from the lectures that by selecting a finite vector of nodes $\\mathbf{x} = (x_1,\\dots,x_N)$, we can define a multivariate random variable $\\mathbf{Y}_0$:\n",
    "$$\n",
    "\\mathbf{Y}_0 = \\mathcal{Y}(\\mathbf{x}) \\sim \\mathcal{N}(\\mu(\\mathbf{x}), P)\n",
    "$$\n",
    "Define a function `mu(x)`, and using this and the previously defined `r()`, generate random samples from $\\mathbf{Y}_0$ for your *test nodes* only.\n",
    "\n",
    "[Note: Use `scipy.stats.multivariate_normal()` with the parmeter `allow_singular=True` to surpress errors and warnings.  This is a numerical conditioning issue, due to floating-point accuracy - and does not result in any problems in this notebook.  Generating (e.g. 10) samples can be done with `scipy.stats.multivariate_normal(mu,P).rvs(10)`]\n",
    "\n",
    "Plot a few (e.g. 10-20) samples of $\\mathbf{Y}_0$ (each sample is a function).  Explore the effects of $\\mu$, $\\sigma_0$ and $l$ on the functions generated.  Do you think one of the *class* of functions generated would be a good fit for $g$?  Find parameters that you think represent a reasonable *prior* for $g()$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data (likelihood)\n",
    "\n",
    "In our Bayesian formulation the random-variable $\\mathbf{Y} | \\mathbf{d}$ (that we are trying to identify), is defined at both training- and test nodes.  Data $\\mathbf{d}\\in \\mathbb{R}^M$ is only available at training nodes.  We need therefore:\n",
    " \n",
    "1. An observation matrix $H \\in \\mathbb{R}^{M\\times (M+N)}$, which extracts only the observed (training) values from the combined training + test vector $\\mathbf{y}$.\n",
    "2. The observation error matrix $R = \\sigma_\\epsilon^2 I\\in \\mathbb{R}^{M\\times M}$ - whereby we assume Gaussian uncorrellated observation errors.  We assume throughout that $\\sigma_\\epsilon=1\\times 10^{-3}$.\n",
    "\n",
    "**EXERCISE 3**\n",
    "\n",
    "Implement 1: There is a useful trick here: Numpy *slices*.  With the syntax `x_all[:M]` you extract the 1st `M` entries of `x_all`.  Similarly `x_all[M:]` returns all except the 1st `M` entries.  Apply this idea to an identity matrix of size $M+N$ to get a suitable matrix $H$.\n",
    "\n",
    "Implement 2. As well as $R$, also evaluate the data $\\mathbf{d} := g(\\mathbf{x}_\\mathrm{train}) + \\epsilon$.  I.e. explicitly add a small about of noise to the observations to simulate a real measurement.  Use `stats.multivariate_normal(0*x_train, R).rvs()` to generate the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 4**\n",
    "\n",
    "Now implementing Kriging is just a question of computing the posterior pdf from Bayes rule:\n",
    "$$\n",
    "p( \\mathbf{y} \\mid \\mathbf{d}) \\propto p(\\mathbf{d}\\mid  \\mathbf{y}) \\cdot p_0(\\mathbf{y}).\n",
    "$$\n",
    "Since prior and likelihood are both Gaussian, the posterior is Gaussian\n",
    "$$\n",
    "\\mathbf{Y} \\mid \\mathbf{d} \\sim \\mathcal{N}(\\hat\\mu, \\hat\\Sigma),\n",
    "$$\n",
    "and we need only find the mean and covariance matrix - which is just linear algebra:\n",
    "$$\n",
    "\\hat\\mu = \\mu_y + K (\\mathbf{d} - H\\mu_y) \\\\\n",
    "\\hat\\Sigma = (I - K H) P \\\\\n",
    "K = PH^T (R+HPH^T)^{-1}.\n",
    "$$\n",
    "Beware that in this formula, the prior mean $\\mu_0$ and covariance $P$ should be evaluated at *all* nodes.\n",
    "\n",
    "Plot the mean, and a few random samples from the posterior.  Compare with the true function.  [For plotting using only the test nodes is convenient.]  You should see a good fit at the training nodes, and closeby - especially where training nodes are clustered.  The fit elsewhere may be poor.  Choose reasonable values for $\\mu()$ and $\\sigma_0$ given what you know about the function - adjust $l$ to try to get a reasonable fit (it may be difficult without more than 10 samples!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 5**\n",
    "\n",
    "Take your code so far, and (without much modification), put in a form which can be called in the following way.  Check results are similar to your previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kriging(M, N, x, d, mu, sigma_0, sigma_epsilon, l):\n",
    "    \"\"\"\n",
    "    Kriging in 1-dimension for a single variable - following the\n",
    "    Bayesian derivation and notation.\n",
    "    \n",
    "    Arguments:\n",
    "      M,N [integers]: Number of training-, test-nodes.\n",
    "      x [array (M+N)]: Node locations (both observations and predictions)\n",
    "      d [array (M)]: Nodal observations (\"data\")\n",
    "      mu [function]: Mean of prior.\n",
    "      sigma_0 [float]: Standard-deviation of the prior.\n",
    "      sigma_epsilon [float]: Standard-deviation of observation error.\n",
    "      l [float]: Correlation length.\n",
    "    Return:\n",
    "      muhat [array (M+N)]: Posterior mean\n",
    "      Sigmahat [array (M+N,M+N)]: Posterior covariance.\n",
    "    \"\"\"\n",
    "    pass  # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence study\n",
    "**EXERCISE 6**\n",
    "\n",
    "The most basic thing we require from numerical methods is *convergence*.  The following code will test the convergence of your `kriging()` function.  For an error measure the RMSE w.r.t. the kriging mean, at the test nodes is used:\n",
    "$$\n",
    "\\epsilon := \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (g(\\mathbf{x}_i) - \\hat\\mu(\\mathbf{x}_i))^2}\n",
    "$$\n",
    "Both training and test-nodes are uniformly spaced to reduce noise.  You can see the variability in convergence by running `convergence_study()` multiple times in the cell below, which will add more lines to the same graph.\n",
    "\n",
    "Which of the 3 parameters $\\mu$, $\\sigma_0$, $l$ is the most important for the convergence rate and absolute error?  What happens to the converenge for large values of $M$?  It should approach a constant *rate*? (I.e. straight-line in log-log plot.)  Is this rate familiar?  Why is does this rate result eventually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence study\n",
    "def mu(x): ???   # TODO pick a reasonable prior\n",
    "sigma_0 = ???\n",
    "l = ???\n",
    "sigma_epsilon = 1.e-3\n",
    "\n",
    "def convergence_study(mu, sigma_0, sigma_epsilon, l):\n",
    "    N = 1000\n",
    "    Ms = 2**np.arange(2,12,dtype=np.int)\n",
    "    err_M = np.zeros(len(Ms))\n",
    "    for j,M in enumerate(Ms):\n",
    "        x = np.hstack((np.linspace(0,1,M), np.linspace(0,1,N)))\n",
    "        d = g(x[:M]) + stats.multivariate_normal(np.zeros(M), np.identity(M)*sigma_epsilon**2).rvs()\n",
    "        muhat,_ = kriging(M, N, x, d, mu, sigma_0, sigma_epsilon, l)\n",
    "        err_M[j] += np.sqrt(np.sum((g(x[M:]) - muhat)**2)/N)\n",
    "\n",
    "    plt.loglog(Ms, err_M, 'x-')\n",
    "\n",
    "    plt.xlabel(r'$M$')\n",
    "    plt.ylabel(r'$\\epsilon$')\n",
    "    slope, intercept, r_value, p_value, std_err = \\\n",
    "        stats.linregress(np.log10(Ms)[-3:], np.log10(err_M)[-3:])\n",
    "    print(\"Convergence rate = %.4g\" % -slope)\n",
    "    plt.loglog(Ms, 10**(intercept + slope*np.log10(Ms)), '--', color='0.5')   \n",
    "    \n",
    "convergence_study(mu, sigma_0, sigma_epsilon, l/4)\n",
    "convergence_study(mu, sigma_0, sigma_epsilon, l/2)\n",
    "convergence_study(mu, sigma_0, sigma_epsilon, l)\n",
    "convergence_study(mu, sigma_0, sigma_epsilon, 2*l)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
